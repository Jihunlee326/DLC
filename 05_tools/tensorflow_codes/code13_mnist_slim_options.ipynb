{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST convolutional neural networks with slim\n",
    "\n",
    "* MNIST data를 가지고 **convolutional neural networks**를 `tf.contrib.slim`을 이용하여 만들어보자.\n",
    "  * [참고: TensorFlow slim](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim)\n",
    "* `tf.contrib.slim`에서 각 `layer`의 옵션들을 직접 컨트롤 해보자.\n",
    "* `l2_regularization`을 사용해보자.\n",
    "* `batch_norm`을 어떻게 사용하는지 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A deep MNIST classifier using convolutional layers.\n",
    "See extensive documentation at\n",
    "https://www.tensorflow.org/get_started/mnist/pros\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "tf.set_random_seed(219)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../mnist'\n",
    "mnist = input_data.read_data_sets(data_dir, one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "index = 1000\n",
    "print(\"label = \", np.argmax(mnist.train.labels[index]))\n",
    "plt.imshow(mnist.train.images[index].reshape(28, 28), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "\n",
    "* [`tf.contrib.layers`](https://www.tensorflow.org/api_guides/python/contrib.layers)\n",
    "* [`tf.contrib.layers.conv2d`](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/conv2d)\n",
    "* [`tf.contrib.layers.max_pool2d`](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/max_pool2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepnn_slim(x):\n",
    "  is_training = tf.placeholder(tf.bool)\n",
    "  batch_norm_params = {'decay': 0.9,\n",
    "                       'epsilon': 0.001,\n",
    "                       'is_training': is_training,\n",
    "                       'scope': 'batch_norm'}\n",
    "  l2_decay = 0.0001\n",
    "  \n",
    "  # Reshape to use within a convolutional neural net.\n",
    "  # Last dimension is for \"features\" - there is only one here, since images are\n",
    "  # grayscale -- it would be 3 for an RGB image, 4 for RGBA, etc.\n",
    "  with tf.name_scope('reshape'):\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "  # First convolutional layer - maps one grayscale image to 32 feature maps.\n",
    "  conv1 = slim.conv2d(inputs=x_image,\n",
    "                      num_outputs=32,\n",
    "                      kernel_size=[5, 5],\n",
    "                      stride=[1, 1],\n",
    "                      padding='SAME',\n",
    "                      activation_fn=tf.nn.relu,\n",
    "                      normalizer_fn=slim.batch_norm,\n",
    "                      normalizer_params=batch_norm_params,\n",
    "                      weights_initializer=slim.xavier_initializer(),\n",
    "                      weights_regularizer=slim.l2_regularizer(l2_decay),\n",
    "                      scope='conv1')\n",
    "    \n",
    "  # Pooling layer - downsamples by 2X.\n",
    "  pool1 = slim.max_pool2d(inputs=conv1,\n",
    "                          kernel_size=[2, 2],\n",
    "                          stride=2,\n",
    "                          padding='VALID',\n",
    "                          scope='pool1')\n",
    "\n",
    "  # Second convolutional layer -- maps 32 feature maps to 64.\n",
    "  conv2 = slim.conv2d(inputs=pool1,\n",
    "                      num_outputs=64,\n",
    "                      kernel_size=[5, 5],\n",
    "                      stride=[1, 1],\n",
    "                      padding='SAME',\n",
    "                      activation_fn=tf.nn.relu,\n",
    "                      normalizer_fn=slim.batch_norm,\n",
    "                      normalizer_params=batch_norm_params,\n",
    "                      weights_initializer=slim.xavier_initializer(),\n",
    "                      weights_regularizer=slim.l2_regularizer(l2_decay),\n",
    "                      scope='conv2')\n",
    "    \n",
    "  # Second pooling layer.\n",
    "  pool2 = slim.max_pool2d(inputs=conv2,\n",
    "                          kernel_size=[2, 2],\n",
    "                          stride=2,\n",
    "                          padding='VALID',\n",
    "                          scope='pool2')\n",
    "\n",
    "  # Fully connected layer 1 -- after 2 round of downsampling, our 28x28 image\n",
    "  # is down to 7x7x64 feature maps -- maps this to 1024 features.\n",
    "  flat = slim.flatten(pool2, scope='flatten')\n",
    "\n",
    "  fc1 = slim.fully_connected(flat,\n",
    "                             num_outputs=1024,\n",
    "                             activation_fn=tf.nn.relu,\n",
    "                             normalizer_fn=slim.batch_norm,\n",
    "                             normalizer_params=batch_norm_params,\n",
    "                             weights_initializer=slim.xavier_initializer(),\n",
    "                             weights_regularizer=slim.l2_regularizer(l2_decay),\n",
    "                             scope='fc1')\n",
    "\n",
    "  # Dropout - controls the complexity of the model, prevents co-adaptation of\n",
    "  # features.\n",
    "  #with tf.name_scope('dropout'):\n",
    "  #keep_prob = tf.placeholder(tf.float32)\n",
    "  #fc1_drop = slim.dropout(fc1, keep_prob)\n",
    "  fc1_drop = slim.dropout(fc1, keep_prob=0.5, is_training=is_training, scope='dropout')\n",
    "\n",
    "  # Map the 1024 features to 10 classes, one for each digit\n",
    "  fc2 = slim.fully_connected(fc1_drop,\n",
    "                             num_outputs=10,\n",
    "                             activation_fn=None,\n",
    "                             normalizer_fn=slim.batch_norm,\n",
    "                             normalizer_params=batch_norm_params,\n",
    "                             weights_initializer=slim.xavier_initializer(),\n",
    "                             weights_regularizer=slim.l2_regularizer(l2_decay),\n",
    "                             scope='fc2')\n",
    "    \n",
    "  return fc2, is_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only necessary if you use IDLE or a jupyter notebook\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model and define loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the graph for the deep net\n",
    "y_conv, is_training = deepnn_slim(x)\n",
    "\n",
    "cross_entropy = tf.losses.softmax_cross_entropy(onehot_labels=y_,\n",
    "                                                logits=y_conv,\n",
    "                                                scope='cross_entropy')\n",
    "l2_regualrization = tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "with tf.name_scope('total_loss'):\n",
    "  total_loss = cross_entropy + l2_regualrization\n",
    "\n",
    "\n",
    "#with tf.name_scope('adam_optimizer'):\n",
    "#  train_step = tf.train.AdamOptimizer(1e-4).minimize(total_loss)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "  correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "  correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "accuracy = tf.reduce_mean(correct_prediction)\n",
    "\n",
    "graph_location = 'graphs/code13_mnist_slim_options'\n",
    "print('Saving graph to: %s' % graph_location)\n",
    "train_writer = tf.summary.FileWriter(graph_location)\n",
    "train_writer.add_graph(tf.get_default_graph())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch normalization update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_collection(tf.GraphKeys.UPDATE_OPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch normalization update\n",
    "batchnorm_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "# Add dependency to compute batchnorm_updates.\n",
    "with tf.control_dependencies(batchnorm_update_ops):\n",
    "  train_step = tf.train.AdamOptimizer(1e-4).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.Session() and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "sess = tf.Session(config=sess_config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "max_step = 100\n",
    "for step in range(max_step+1):\n",
    "  batch = mnist.train.next_batch(32)\n",
    "  feed_dict_train = {x: batch[0],\n",
    "                     y_: batch[1],\n",
    "                     is_training: True}\n",
    "  feed_dict_eval = {x: batch[0],\n",
    "                    y_: batch[1],\n",
    "                    is_training: False}\n",
    "  _, tl, ce = sess.run([train_step, total_loss, cross_entropy], feed_dict=feed_dict_train)\n",
    "  if step % 10 == 0:\n",
    "    train_accuracy = sess.run(accuracy, feed_dict=feed_dict_eval)\n",
    "    print('step %d, training accuracy %g, total_loss %g, cross_entropy %g' % (step, train_accuracy, tl, ce))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test trained model\n",
    "\n",
    "* test accuracy: 0.9378"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test accuracy %g' % sess.run(accuracy, feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, is_training: False}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test images inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "test_batch_size = 16\n",
    "batch_xs, _ = mnist.test.next_batch(test_batch_size)\n",
    "y_pred = sess.run(y_conv, feed_dict={x: batch_xs,\n",
    "                                     is_training: False})\n",
    "  \n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "for i, (px, py) in enumerate(zip(batch_xs, y_pred)):\n",
    "  p = fig.add_subplot(4, 8, i+1)\n",
    "  p.set_title(\"y_pred: {}\".format(np.argmax(py)))\n",
    "  p.imshow(px.reshape(28, 28), cmap='gray')\n",
    "  p.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print all trainable variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.global_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.model_variables()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
