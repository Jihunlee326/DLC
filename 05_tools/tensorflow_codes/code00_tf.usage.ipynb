{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Usage\n",
    "\n",
    "**TensorFlow**는 구글에서 만든 open source software library이다.\n",
    "Data flow graphs를 이용하여 numerical 계산을 수행할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Installation\n",
    "\n",
    "#### Basic\n",
    "* official site (<https://tensorflow.org>)\n",
    "\n",
    "#### Requirements\n",
    "* Environments: Ubuntu 16.04\n",
    "* python 2.7 or 3.6\n",
    "* python-pip\n",
    "* python-numpy\n",
    "* anaconda\n",
    "* Cuda Toolkit 8.0 (optional)\n",
    "* cuDNN v6 (optional)\n",
    "\n",
    "#### Install TensorFlow\n",
    "```shell\n",
    "$ sudo pip3 install tensorflow       # Python 3.6; CPU only\n",
    "$ sudo pip3 install tensorflow-gpu   # Python 3.6; GPU support\n",
    "```\n",
    "* more information about installation (<https://www.tensorflow.org/install/>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 자신의 코드를 작성하는 법\n",
    "\n",
    "### Flow of general machine learning\n",
    "* 풀고자 하는 문제가 supervised-learning인지 unsupervised-learning인지 파악한다.\n",
    "* Supervised-learning일 경우 적절할 데이터 셋을 모아야 한다.\n",
    "  * input data -- label 이 쌍으로 존재하는 데이터 셋을 찾아서 모아야 한다.\n",
    "  * 데이터는 많으면 많을수록 좋다\n",
    "* model을 구현한다.\n",
    "* Train (and validation)\n",
    "* Test\n",
    "* Service (deploy model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning flow를 따라 구현하기\n",
    "\n",
    "#### 참고할 만한 좋은 reference codes\n",
    "\n",
    "* cifar10 code (<https://www.tensorflow.org/tutorials/deep_cnn>)\n",
    "* inception code (<https://github.com/tensorflow/models/tree/master/research/inception>)\n",
    "* show_and_tell code (<https://github.com/tensorflow/models/tree/master/research/im2txt>)\n",
    "* other modles (<https://github.com/tensorflow/models/>)\n",
    "* **official pretrained model** (<https://github.com/tensorflow/models/tree/master/research/slim>)\n",
    "* **official API docs** (<https://www.tensorflow.org/api_docs/>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Various high-level wrapper\n",
    "\n",
    "* [`tf.contrib.slim`](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim)\n",
    "* [`keras`](https://keras.io/)\n",
    "* ...\n",
    "* 저는 주로 `tf.contrib.slim`을 씁니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 0. Commons\n",
    "\n",
    "* 크게 파일을 여섯 가지 형태로 구분하고 각각을 구현한다.\n",
    "  * `train.py`\n",
    "    * 실제 학습에 필요한 flow를 구현\n",
    "  * `eval.py`\n",
    "    * 학습된 모델을 평가하는 flow를 구현\n",
    "  * `model.py`\n",
    "    * 만들고자 하는 모델 구현\n",
    "  * `inference.py`\n",
    "    * service 환경처럼 개별 데이터 inference하는 코드 구현\n",
    "  * `image_processiong.py`\n",
    "    * Data 입출력 및 augmentation에 관련된 코드 구현\n",
    "  * `configuration.py`\n",
    "    * Hyper-parameter들 모아놓은 코드\n",
    "* first import (TensorFlow recommendation)\n",
    "\n",
    "```python\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "```\n",
    "\n",
    "* second import (우리가 필요한 library들)\n",
    "\n",
    "```python\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "slim = tf.contrib.slim  # my recommendation\n",
    "```\n",
    "\n",
    "* Set Flags (hyper-parameters) **요즘은 parser로 바뀐 것 같음**\n",
    "\n",
    "```python\n",
    "tf.app.flags.DEFINE_string('train_dir',\n",
    "                           'my_dir',\n",
    "                           'Directory where checkpoints and event logs are written to.')\n",
    "tf.app.flags.DEFINE_integer()\n",
    "tf.app.flags.DEFINE_float()\n",
    "tf.app.flags.DEFINE_boolean()\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. Train\n",
    "\n",
    "* pseudo code (flow를 나타낸다.)\n",
    "\n",
    "```python\n",
    "with tf.Graph().as_default():\n",
    "  # Build the model\n",
    "\n",
    "  # Create global step\n",
    "  # Calculate the learning rate schedule\n",
    "  # Decay the learning rate exponentially based on the number of steps\n",
    "\n",
    "  # Create an optimizer\n",
    "  # Minimize optimizer\n",
    "\n",
    "  # Set up the Saver\n",
    "\n",
    "  # Start session\n",
    "  with tf.Session() as sess:\n",
    "    # start the queue runners\n",
    "\n",
    "    # Create a summary writer\n",
    "    # Build the summary operation\n",
    "\n",
    "    for step in range(FLAGS.max_step):\n",
    "      _, loss = sess.run([opt_op, model.loss])\n",
    "      # Print loss and save the model checkpoints periodically\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the model\n",
    "\n",
    "```python\n",
    "import model\n",
    "\n",
    "model = model.MyModel(mode=\"train\")\n",
    "model.build()\n",
    "```\n",
    "\n",
    "모델 코드 (`model.py`) 작성은 다음 2. Model에서 다룸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create global step\n",
    "\n",
    "```python\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "```\n",
    "\n",
    "`tf.train`을 이용하여 `global_step`을 생성한다.\n",
    "`global_step`은 현재 학습상태가 전체적으로 봤을때 몇 번째 step인지 저장하고 있다.\n",
    "이를 이용하여 learning rate decay schedule에 이용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the learning rate schedule\n",
    "\n",
    "```python\n",
    "num_batch_per_epoch = (FLAGS.num_examples / FLAGS.batch_size)\n",
    "decay_steps = int(num_batch_per_epoch * FLAGS.num_epochs_per_decay)\n",
    "```\n",
    "\n",
    "FLAGS(hyper-parameters들의 argument)를 이용하여 learning rate를 몇 스텝에 한번씩\n",
    "decay할지 결정함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decay the learning rate exponentially based on the number of steps\n",
    "\n",
    "```python\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "      FLAGS.initial_learning_rate,\n",
    "      global_step,\n",
    "      decay_steps=decay_steps,\n",
    "      decay_rate=FLAGS.learning_rate_decay_factor,\n",
    "      staircase=True)\n",
    "```\n",
    "\n",
    "[`tf.train`](https://www.tensorflow.org/api_guides/python/train) 모듈에는 train에 필요한\n",
    "많은 api들이 제공된다.\n",
    "다양한 종류의 learning rate decay 함수 부터, 바로 다음에 나올 Optimizer, batch 함수,\n",
    "file 입출력, model save 관련 함수들이 있다.\n",
    "* `tf.train.exponential_decay`\n",
    "* `tf.train.inverse_time_decay`\n",
    "* `tf.train.natural_exp_decay`\n",
    "* `tf.train.piecewise_constant`\n",
    "* `tf.train.polynomial_decay`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an optimizer and minimize optimizer\n",
    "\n",
    "```python\n",
    "opt = tf.train.AdamOptimizer(learning_rate)\n",
    "opt_op = opt.minimize(model.loss,\n",
    "                      global_step=global_step)\n",
    "```\n",
    "\n",
    "[`tf.train.Optimizer`](https://www.tensorflow.org/api_docs/python/tf/train/Optimizer) 모듈에는\n",
    "여러가지 optimizer 함수들이 제공된다.\n",
    "* `tf.train.Optimizer`\n",
    "* `tf.train.GradientDescentOptimizer`\n",
    "* `tf.train.AdadeltaOptimizer`\n",
    "* `tf.train.AdagradOptimizer`\n",
    "* `tf.train.AdagradDAOptimizer`\n",
    "* `tf.train.MomentumOptimizer`\n",
    "* `tf.train.AdamOptimizer`\n",
    "* `tf.train.FtrlOptimizer`\n",
    "* `tf.train.ProximalGradientDescentOptimizer`\n",
    "* `tf.train.ProximalAdagradOptimizer`\n",
    "* `tf.train.RMSPropOptimizer`\n",
    "\n",
    "optimizer는 기본적으로 `learning_rate` argument가 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the Saver\n",
    "\n",
    "```python\n",
    "# Create a saver\n",
    "saver = tf.train.Saver(max_to_keep=1000)\n",
    "\n",
    "# Save the model\n",
    "with tf.Session() as sess:\n",
    "  for step in xrange(10000):\n",
    "\tsess.run(opt_op)\n",
    "\t  if step % 100 == 0:\n",
    "\t\tsaver.save(sess, 'my-model', global_step=step)\n",
    "```\n",
    "\n",
    "[`tf.train.Saver`](https://www.tensorflow.org/api_docs/python/tf/train/Saver) 모델을 저장 또는\n",
    "불러올때 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start session\n",
    "\n",
    "```python\n",
    "# Build an initialization operation to run below\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start running operations on the Graph\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "```\n",
    "\n",
    "위와 같이 session을 생성할 수 있다.\n",
    "TensorFlow는 `Session`개념이 굉장히 중요하고 이것이 일반적인 프로그래밍과는 다른 차별점을 준다.\n",
    "사실 이 부분 때문에 초보자들이 프로그래밍과 디버깅하기 어려운 것 같다.\n",
    "`sess = tf.Session ()`이 생성되기 전까지 위의 프로그래밍은 사실 세팅만 한 것에 불과하다.\n",
    "저 위의 코드들은 마치 파이프 라인만 연결해 놓은 상태인 것이다.\n",
    "session을 열고 실행할 때 물이 흐르고 실제로 모든 계산이 시작된다.\n",
    "위의 코드는 간단히 `with`문으로 구현할 수 있다.\n",
    "\n",
    "```python\n",
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### start the queue runners\n",
    "```phthon\n",
    "tf.train.start_queue_runners(sess=sess)\n",
    "```\n",
    "[`Threading and Queues`](https://www.tensorflow.org/programmers_guide/threading_and_queues)\n",
    "페이지를 참조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a summary writer and build the summary operation\n",
    "\n",
    "```python\n",
    "# Create a summary writer\n",
    "summary_writer = tf.summary.FileWriter(FLAGS.train_dir, sess.graph)\n",
    "\n",
    "# Build the summary operation\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "# Write summary events periodically\n",
    "with tf.Session() as sess:\n",
    "  for step in xrange(10000):\n",
    "    sess.run(opt_op)\n",
    "    if step % 200 == 0:\n",
    "      summayr_str = sess.run(summary_op)\n",
    "      summary_writer.add_summary(summary_str, step)\n",
    "```\n",
    "\n",
    "[`tf.summary`](https://www.tensorflow.org/api_docs/python/tf/summary) 페이지 참조.\n",
    "`tf.summary.FileWriter`를 생성하고, summary operation `summary_op`으로\n",
    "학습관련 summary들을 다 모은다.\n",
    "그리고 `Session`에서 원하는 주기마다 `summary_writer`에 summary 저장."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute loss\n",
    "\n",
    "```python\n",
    "for step in range(FLAGS.max_step):\n",
    "  _, loss = sess.run([opt_op, model.loss])\n",
    "  # Print loss and save the model checkpoints periodically\n",
    "  if step % 10 == 0:\n",
    "    print('step: %d  loss: %f' % (step, loss))\n",
    "\n",
    "  if step % FLAGS.save_steps == 0:\n",
    "    checkpoint_path = os.path.join(FLAGS.train_dir, 'model.ckpt')\n",
    "    saver.save(sess, checkpoint_path, global_step=step)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. Model\n",
    "\n",
    "* 주로 모델을 구현할 때 `tf.contrib.slim` high-level wrapper를 사용한다.\n",
    "* 모델을 하나의 클래스로 만들고 보통 다섯가지 기본 module들을 만든다.\n",
    "  * `__init__`\n",
    "  * `build_inputs`\n",
    "  * `build_network`\n",
    "  * `build_model` (for loss computation)\n",
    "  * `build`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  `__init__`\n",
    "\n",
    "* 초기화에 필요한 변수들을 argument 형태로 넣는다.\n",
    "  * `mode in ['train', 'val', 'inference']`\n",
    "  * `FLAGS`형태의 global variable도 모델안에서 쓸 수 있게 `self` 변수로 저장한다.\n",
    "  * 반드시 필요한 변수인 `loss`, `global_step`등도 `self`로 가지고 있는다.\n",
    "  \n",
    "\n",
    "#### `build_inputs`\n",
    "\n",
    "* train, validation에서는 tensor형태로 변수를 선언하고\n",
    "  inference 모드에서는 외부에서 받을 수 있게 `tf.placeholder`형태로 선언한다.\n",
    "  \n",
    "\n",
    "#### `build_network`\n",
    "\n",
    "* 열심히 만든다.\n",
    "* 구조를 고려하여 `variable_scope`로 적절히 묶어주자.\n",
    "* 레이어마다 공통으로 쓰이는 hyper parameter들은 `arg_scope`로 묶어서 사용하자.\n",
    "* 맨 마지막 레이어는 `loss`를 계산하기 위한 `logits`형태로 남겨두자 (**NO activation**)\n",
    "\n",
    "\n",
    "#### `build_model`\n",
    "\n",
    "* train, validation, inference 상황에 맞는 `loss`, `eval` 또는 `probability`값을 선언하자.\n",
    "\n",
    "\n",
    "#### `build`\n",
    "\n",
    "* 모든 build 단계를 합친다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
